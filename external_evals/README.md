# Local End-to-End Evaluation Framework

A unified framework to run **LiveBench**, **TruthfulQA**, **IFEval**, and **AlpacaEval2** on locally hosted models (e.g., using vLLM).

**ğŸ“– è¯¦ç»†æ–‡æ¡£**: è¯·å‚é˜… [EVALUATION_FRAMEWORK.md](./EVALUATION_FRAMEWORK.md) äº†è§£æ¯ä¸ªè¯„ä¼°çš„å·¥ä½œæµç¨‹å’Œæ•°æ®æµã€‚

---

## å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ç¯å¢ƒ

```bash
conda create -n learnarena python=3.10
conda activate learnarena
pip install -r external_evals/tools/requirements.txt
```

### 2. è¿è¡Œè¯„ä¼°

```bash
# æ–¹å¼ A: è¯„ä¼°å•ä¸ª HuggingFace æ¨¡å‹
./external_evals/tools/run_batch_generate.sh Qwen/Qwen2.5-0.5B-Instruct
./external_evals/tools/run_batch_evaluate.sh Qwen2.5-0.5B-Instruct

# æ–¹å¼ B: è¯„ä¼° SFT è®­ç»ƒäº§ç”Ÿçš„å¤šä¸ª checkpoint
./external_evals/tools/run_batch_generate.sh /path/to/checkpoints
./external_evals/tools/run_batch_evaluate.sh /path/to/checkpoints

# æ–¹å¼ C: åªè¯„ä¼°ç‰¹å®šçš„ benchmarks
./external_evals/tools/run_batch_generate.sh Qwen/Qwen2.5-0.5B-Instruct --benchmarks ifeval,truthfulqa
```

---

## å››ç§è¯„ä¼°åŸºå‡†

| Benchmark | ç›®çš„ | è¾“å…¥ | ç”Ÿæˆè¾“å‡º | è¯„ä¼°è¾“å‡º |
|-----------|------|------|----------|----------|
| **IFEval** | æŒ‡ä»¤è·Ÿéš | 541 æ¡å¤æ‚æŒ‡ä»¤ | `candidate_outputs.jsonl` | `eval_results_*.jsonl` |
| **TruthfulQA** | çœŸå®æ€§é—®ç­” | 790 ä¸ªé—®é¢˜ | `TruthfulQA_generated.csv` | `results.csv` |
| **AlpacaEval2** | LLM-as-Judge | 805 ä¸ªæŒ‡ä»¤ | `model_outputs.json` | `annotations.json` |
| **LiveBench** | å¤šé¢†åŸŸè¯„ä¼° | å¤šé¢†åŸŸé—®é¢˜ | `model_answers/` | `model_judgment/` |

---

## è„šæœ¬ä½¿ç”¨è¯´æ˜

### run_batch_generate.sh - ç”Ÿæˆé˜¶æ®µ

```bash
./run_batch_generate.sh <model_input> [options]
```

**Model Input ç±»å‹:**
- æ–‡ä»¶å¤¹è·¯å¾„: `/path/to/checkpoints` (éå†æ‰€æœ‰å­ç›®å½•)
- HuggingFace æ¨¡å‹: `Qwen/Qwen2.5-0.5B-Instruct`
- æœ¬åœ°æ¨¡å‹è·¯å¾„: `/root/models/my-model`

**é€‰é¡¹:**
| é€‰é¡¹ | è¯´æ˜ | é»˜è®¤å€¼ |
|------|------|--------|
| `-p, --port` | vLLM æœåŠ¡ç«¯å£ | 8001 |
| `-g, --gpu-mem` | GPU å†…å­˜ä½¿ç”¨ç‡ | 0.85 |
| `-t, --tp-size` | Tensor Parallel å¤§å° | 1 |
| `-b, --benchmarks` | è¦è¿è¡Œçš„ benchmarks | ifeval,truthfulqa,alpacaeval,livebench |
| `--skip-existing` | è·³è¿‡å·²æœ‰ç»“æœ | false |

**ç¤ºä¾‹:**
```bash
# è¯„ä¼° checkpoints æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ¨¡å‹
./run_batch_generate.sh /root/checkpoints

# è¯„ä¼°å•ä¸ª HuggingFace æ¨¡å‹
./run_batch_generate.sh Qwen/Qwen2.5-0.5B-Instruct

# åªè¿è¡Œç‰¹å®š benchmarks
./run_batch_generate.sh Qwen/Qwen2.5-0.5B-Instruct --benchmarks ifeval,truthfulqa

# è‡ªå®šä¹‰ç«¯å£å’Œ GPU
./run_batch_generate.sh /root/models/my-model --port 8002 --gpu-mem 0.9
```

### run_batch_evaluate.sh - è¯„ä¼°é˜¶æ®µ

```bash
./run_batch_evaluate.sh <model_input> [options]
```

**é€‰é¡¹:**
| é€‰é¡¹ | è¯´æ˜ | é»˜è®¤å€¼ |
|------|------|--------|
| `-b, --benchmarks` | è¦è¯„ä¼°çš„ benchmarks | ifeval,truthfulqa,alpacaeval,livebench |
| `--judge-api-base` | Judge API åœ°å€ | OpenRouter |
| `--judge-api-key` | Judge API å¯†é’¥ | - |
| `--judge-model` | Judge æ¨¡å‹åç§° | stepfun/step-3.5-flash:free |
| `--skip-existing` | è·³è¿‡å·²æœ‰è¯„ä¼°ç»“æœ | false |

**ç¤ºä¾‹:**
```bash
# è¯„ä¼° checkpoints æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰ç»“æœ
./run_batch_evaluate.sh /root/checkpoints

# è¯„ä¼°å•ä¸ªæ¨¡å‹çš„ç»“æœ
./run_batch_evaluate.sh Qwen2.5-0.5B-Instruct

# ä½¿ç”¨æœ¬åœ° Judge æ¨¡å‹
./run_batch_evaluate.sh /root/checkpoints \
  --judge-api-base http://localhost:8000/v1 \
  --judge-model Qwen/Qwen2.5-72B-Instruct

# åªè¯„ä¼°ç‰¹å®š benchmarks
./run_batch_evaluate.sh Qwen2.5-0.5B-Instruct --benchmarks ifeval,truthfulqa
```

---

## ç»“æœä½ç½®

```
external_evals/results/
â””â”€â”€ <model_id>/
    â”œâ”€â”€ ifeval/
    â”‚   â”œâ”€â”€ candidate_outputs.jsonl        # ç”Ÿæˆç»“æœ
    â”‚   â”œâ”€â”€ eval_results_strict.jsonl      # è¯„ä¼°ç»“æœ (ä¸¥æ ¼)
    â”‚   â””â”€â”€ eval_results_loose.jsonl       # è¯„ä¼°ç»“æœ (å®½æ¾)
    â”‚
    â”œâ”€â”€ truthfulqa/
    â”‚   â”œâ”€â”€ TruthfulQA_generated.csv       # ç”Ÿæˆç»“æœ
    â”‚   â””â”€â”€ results.csv                    # è¯„ä¼°ç»“æœ
    â”‚
    â””â”€â”€ alpacaeval2/
        â””â”€â”€ model_outputs.json             # ç”Ÿæˆç»“æœ

external_evals/alpaca_eval/results/
â””â”€â”€ <model_id>/
    â”œâ”€â”€ annotations.json                   # è¯„ä¼°ç»“æœ (è¯¦ç»†)
    â””â”€â”€ leaderboard.csv                    # è¯„ä¼°ç»“æœ (æ±‡æ€»)

external_evals/livebench/livebench/
â”œâ”€â”€ model_answers/
â”‚   â””â”€â”€ <model_id>/
â”‚       â””â”€â”€ <category>.jsonl               # ç”Ÿæˆç»“æœ
â””â”€â”€ model_judgment/
    â””â”€â”€ <model_id>/
        â””â”€â”€ <category>.jsonl               # è¯„ä¼°ç»“æœ
```

---

## å…¸å‹å·¥ä½œæµç¨‹

### åœºæ™¯ 1: è¯„ä¼° SFT è®­ç»ƒçš„å¤šä¸ª checkpoint

```bash
# å‡è®¾ä½ çš„ checkpoint ç»“æ„å¦‚ä¸‹:
# /root/sft_checkpoints/
#   â”œâ”€â”€ step-100/
#   â”œâ”€â”€ step-200/
#   â””â”€â”€ step-300/

# Step 1: ç”Ÿæˆæ‰€æœ‰ checkpoint çš„å›ç­”
./external_evals/tools/run_batch_generate.sh /root/sft_checkpoints

# Step 2: è¯„ä¼°æ‰€æœ‰å›ç­”
./external_evals/tools/run_batch_evaluate.sh /root/sft_checkpoints
```

### åœºæ™¯ 2: è¯„ä¼°åŸå§‹é¢„è®­ç»ƒæ¨¡å‹

```bash
# ç›´æ¥ä½¿ç”¨ HuggingFace æ¨¡å‹åç§°
./external_evals/tools/run_batch_generate.sh Qwen/Qwen2.5-0.5B-Instruct
./external_evals/tools/run_batch_evaluate.sh Qwen2.5-0.5B-Instruct
```

### åœºæ™¯ 3: å¿«é€Ÿæµ‹è¯• (åªè·‘ IFEval)

```bash
./external_evals/tools/run_batch_generate.sh Qwen/Qwen2.5-0.5B-Instruct --benchmarks ifeval
./external_evals/tools/run_batch_evaluate.sh Qwen2.5-0.5B-Instruct --benchmarks ifeval
```

---

## é…ç½®æ–‡ä»¶æ¨¡å¼ (é«˜çº§ç”¨æ³•)

å¦‚æœéœ€è¦æ›´ç»†ç²’åº¦çš„æ§åˆ¶ï¼Œå¯ä»¥åˆ›å»ºé…ç½®æ–‡ä»¶:

```yaml
models:
  - id: "my-model"
    api_name: "my-model"
    api_base: "http://localhost:8001/v1"
    api_key: "dummy"

judge:
  api_base: "https://openrouter.ai/api/v1"
  api_key: "sk-or-v1-xxx"
  model_name: "stepfun/step-3.5-flash:free"

benchmarks:
  ifeval: true
  truthfulqa: true
  alpacaeval: true
  livebench: true
```

```bash
# ä½¿ç”¨é…ç½®æ–‡ä»¶
python external_evals/tools/run_evals.py config.yaml --phase all
```
